; Listing generated by Microsoft (R) Optimizing Compiler Version 16.00.40219.01 

	TITLE	C:\Users\Phail\Documents\GitHub\MollenOS\clib\src\string\memmove.c
	.686P
	.XMM
	include listing.inc
	.model	flat

INCLUDELIB MSVCRT
INCLUDELIB OLDNAMES

PUBLIC	_memmove
; Function compile flags: /Ogtp
; File c:\users\phail\documents\github\mollenos\clib\src\string\memmove.c
;	COMDAT _memmove
_TEXT	SEGMENT
_destination$ = 8					; size = 4
_source$ = 12						; size = 4
_count$ = 16						; size = 4
_memmove PROC						; COMDAT

; 11   : {

  00000	55		 push	 ebp
  00001	8b ec		 mov	 ebp, esp

; 12   : 	char *dst = (char *)destination;

  00003	8b 45 08	 mov	 eax, DWORD PTR _destination$[ebp]

; 13   : 	const char *src = (char *)source;

  00006	8b 55 0c	 mov	 edx, DWORD PTR _source$[ebp]
  00009	53		 push	 ebx
  0000a	56		 push	 esi

; 14   : 	long *aligned_dst;
; 15   : 	const long *aligned_src;
; 16   : 
; 17   : 	if (src < dst && dst < src + count)

  0000b	8b 75 10	 mov	 esi, DWORD PTR _count$[ebp]
  0000e	57		 push	 edi
  0000f	8b c8		 mov	 ecx, eax
  00011	3b d0		 cmp	 edx, eax
  00013	73 1f		 jae	 SHORT $LN11@memmove
  00015	8d 3c 32	 lea	 edi, DWORD PTR [edx+esi]
  00018	3b c7		 cmp	 eax, edi
  0001a	73 18		 jae	 SHORT $LN11@memmove

; 18   : 	{
; 19   : 		/* Destructive overlap...have to copy backwards */
; 20   : 		src += count;

  0001c	8b cf		 mov	 ecx, edi

; 21   : 		dst += count;

  0001e	8d 14 30	 lea	 edx, DWORD PTR [eax+esi]

; 22   : 		while (count--)

  00021	85 f6		 test	 esi, esi
  00023	74 6c		 je	 SHORT $LN22@memmove
$LL10@memmove:

; 23   : 	{
; 24   : 		*--dst = *--src;

  00025	8a 59 ff	 mov	 bl, BYTE PTR [ecx-1]
  00028	49		 dec	 ecx
  00029	4a		 dec	 edx
  0002a	4e		 dec	 esi
  0002b	88 1a		 mov	 BYTE PTR [edx], bl
  0002d	75 f6		 jne	 SHORT $LL10@memmove
  0002f	5f		 pop	 edi
  00030	5e		 pop	 esi
  00031	5b		 pop	 ebx

; 62   : 		}
; 63   : 	}
; 64   : 
; 65   : 	return destination;
; 66   : }

  00032	5d		 pop	 ebp
  00033	c3		 ret	 0
$LN11@memmove:

; 25   : 	}
; 26   : 	}
; 27   : 	else
; 28   : 	{
; 29   : 		/* Use optimizing algorithm for a non-destructive copy to closely 
; 30   : 			match memcpy. If the size is small or either SRC or DST is unaligned,
; 31   : 			then punt into the byte copy loop.  This should be rare.  */
; 32   : 		if (!TOO_SMALL(count) && !UNALIGNED (src, dst))

  00034	83 fe 10	 cmp	 esi, 16			; 00000010H
  00037	72 49		 jb	 SHORT $LN17@memmove
  00039	0b c2		 or	 eax, edx
  0003b	a8 03		 test	 al, 3

; 33   : 		{
; 34   : 			aligned_dst = (long*)dst;

  0003d	8b c1		 mov	 eax, ecx
  0003f	75 41		 jne	 SHORT $LN17@memmove
  00041	8b fe		 mov	 edi, esi
  00043	c1 ef 04	 shr	 edi, 4
$LL6@memmove:

; 35   : 			aligned_src = (long*)src;
; 36   : 
; 37   : 			/* Copy 4X long words at a time if possible.  */
; 38   : 			while (count >= BIGBLOCKSIZE)
; 39   : 			{
; 40   : 				*aligned_dst++ = *aligned_src++;

  00046	8b 1a		 mov	 ebx, DWORD PTR [edx]
  00048	89 19		 mov	 DWORD PTR [ecx], ebx

; 41   : 				*aligned_dst++ = *aligned_src++;

  0004a	8b 5a 04	 mov	 ebx, DWORD PTR [edx+4]
  0004d	89 59 04	 mov	 DWORD PTR [ecx+4], ebx

; 42   : 				*aligned_dst++ = *aligned_src++;

  00050	8b 5a 08	 mov	 ebx, DWORD PTR [edx+8]
  00053	89 59 08	 mov	 DWORD PTR [ecx+8], ebx

; 43   : 				*aligned_dst++ = *aligned_src++;

  00056	8b 5a 0c	 mov	 ebx, DWORD PTR [edx+12]
  00059	89 59 0c	 mov	 DWORD PTR [ecx+12], ebx
  0005c	83 c1 10	 add	 ecx, 16			; 00000010H
  0005f	83 c2 10	 add	 edx, 16			; 00000010H

; 44   : 				count -= BIGBLOCKSIZE;

  00062	83 ee 10	 sub	 esi, 16			; 00000010H
  00065	4f		 dec	 edi
  00066	75 de		 jne	 SHORT $LL6@memmove

; 45   : 			}
; 46   : 
; 47   : 			/* Copy one long word at a time if possible.  */
; 48   : 			while (count >= LITTLEBLOCKSIZE)

  00068	83 fe 04	 cmp	 esi, 4
  0006b	72 15		 jb	 SHORT $LN17@memmove
  0006d	8b fe		 mov	 edi, esi
  0006f	c1 ef 02	 shr	 edi, 2
$LL4@memmove:

; 49   : 			{
; 50   : 				*aligned_dst++ = *aligned_src++;

  00072	8b 1a		 mov	 ebx, DWORD PTR [edx]
  00074	89 19		 mov	 DWORD PTR [ecx], ebx
  00076	83 c1 04	 add	 ecx, 4
  00079	83 c2 04	 add	 edx, 4

; 51   : 				count -= LITTLEBLOCKSIZE;

  0007c	83 ee 04	 sub	 esi, 4
  0007f	4f		 dec	 edi
  00080	75 f0		 jne	 SHORT $LL4@memmove
$LN17@memmove:

; 52   : 			}
; 53   : 
; 54   : 			/* Pick up any residual with a byte copier.  */
; 55   : 			dst = (char*)aligned_dst;
; 56   : 			src = (char*)aligned_src;
; 57   : 		}
; 58   : 
; 59   : 		while (count--)

  00082	85 f6		 test	 esi, esi
  00084	74 0b		 je	 SHORT $LN22@memmove
  00086	2b d1		 sub	 edx, ecx
$LL2@memmove:

; 60   : 		{
; 61   : 			*dst++ = *src++;

  00088	8a 1c 0a	 mov	 bl, BYTE PTR [edx+ecx]
  0008b	88 19		 mov	 BYTE PTR [ecx], bl
  0008d	41		 inc	 ecx
  0008e	4e		 dec	 esi
  0008f	75 f7		 jne	 SHORT $LL2@memmove
$LN22@memmove:
  00091	5f		 pop	 edi
  00092	5e		 pop	 esi
  00093	5b		 pop	 ebx

; 62   : 		}
; 63   : 	}
; 64   : 
; 65   : 	return destination;
; 66   : }

  00094	5d		 pop	 ebp
  00095	c3		 ret	 0
_memmove ENDP
_TEXT	ENDS
END
