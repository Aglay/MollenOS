; Listing generated by Microsoft (R) Optimizing Compiler Version 18.00.30501.0 

	TITLE	C:\Users\Phail\Documents\GitHub\MollenOS\clib\src\string\memmove.c
	.686P
	.XMM
	include listing.inc
	.model	flat

INCLUDELIB MSVCRT
INCLUDELIB OLDNAMES

PUBLIC	_memmove
; Function compile flags: /Ogtp
; File c:\users\phail\documents\github\mollenos\clib\src\string\memmove.c
;	COMDAT _memmove
_TEXT	SEGMENT
_destination$ = 8					; size = 4
tv597 = 12						; size = 4
_source$ = 12						; size = 4
_count$ = 16						; size = 4
_memmove PROC						; COMDAT

; 11   : {

  00000	55		 push	 ebp
  00001	8b ec		 mov	 ebp, esp

; 12   : 	char *dst = (char *)destination;
; 13   : 	const char *src = (char *)source;
; 14   : 	long *aligned_dst;
; 15   : 	const long *aligned_src;
; 16   : 
; 17   : 	if (src < dst && dst < src + count)

  00003	8b 4d 10	 mov	 ecx, DWORD PTR _count$[ebp]
  00006	53		 push	 ebx
  00007	8b 5d 08	 mov	 ebx, DWORD PTR _destination$[ebp]
  0000a	8b d3		 mov	 edx, ebx
  0000c	56		 push	 esi
  0000d	8b 75 0c	 mov	 esi, DWORD PTR _source$[ebp]
  00010	57		 push	 edi
  00011	3b f3		 cmp	 esi, ebx
  00013	73 28		 jae	 SHORT $LN11@memmove
  00015	8d 3c 0e	 lea	 edi, DWORD PTR [esi+ecx]
  00018	3b df		 cmp	 ebx, edi
  0001a	73 21		 jae	 SHORT $LN11@memmove

; 18   : 	{
; 19   : 		/* Destructive overlap...have to copy backwards */
; 20   : 		src += count;
; 21   : 		dst += count;

  0001c	8d 34 0b	 lea	 esi, DWORD PTR [ebx+ecx]

; 22   : 		while (count--)

  0001f	85 c9		 test	 ecx, ecx
  00021	0f 84 85 00 00
	00		 je	 $LN45@memmove
$LL10@memmove:

; 23   : 	{
; 24   : 		*--dst = *--src;

  00027	8a 57 ff	 mov	 dl, BYTE PTR [edi-1]
  0002a	8d 7f ff	 lea	 edi, DWORD PTR [edi-1]
  0002d	88 56 ff	 mov	 BYTE PTR [esi-1], dl
  00030	8d 76 ff	 lea	 esi, DWORD PTR [esi-1]
  00033	49		 dec	 ecx
  00034	75 f1		 jne	 SHORT $LL10@memmove

; 62   : 		}
; 63   : 	}
; 64   : 
; 65   : 	return destination;

  00036	5f		 pop	 edi
  00037	5e		 pop	 esi
  00038	8b c3		 mov	 eax, ebx
  0003a	5b		 pop	 ebx

; 66   : }

  0003b	5d		 pop	 ebp
  0003c	c3		 ret	 0
$LN11@memmove:

; 25   : 	}
; 26   : 	}
; 27   : 	else
; 28   : 	{
; 29   : 		/* Use optimizing algorithm for a non-destructive copy to closely 
; 30   : 			match memcpy. If the size is small or either SRC or DST is unaligned,
; 31   : 			then punt into the byte copy loop.  This should be rare.  */
; 32   : 		if (!TOO_SMALL(count) && !UNALIGNED (src, dst))

  0003d	83 f9 10	 cmp	 ecx, 16			; 00000010H
  00040	72 53		 jb	 SHORT $LN3@memmove
  00042	8b c3		 mov	 eax, ebx
  00044	0b c6		 or	 eax, esi
  00046	a8 03		 test	 al, 3
  00048	75 4b		 jne	 SHORT $LN3@memmove

; 33   : 		{
; 34   : 			aligned_dst = (long*)dst;
; 35   : 			aligned_src = (long*)src;
; 36   : 
; 37   : 			/* Copy 4X long words at a time if possible.  */
; 38   : 			while (count >= BIGBLOCKSIZE)

  0004a	8b c6		 mov	 eax, esi
  0004c	8b f9		 mov	 edi, ecx
  0004e	2b c3		 sub	 eax, ebx
  00050	c1 ef 04	 shr	 edi, 4
  00053	8b d8		 mov	 ebx, eax
$LL6@memmove:

; 39   : 			{
; 40   : 				*aligned_dst++ = *aligned_src++;

  00055	8b 06		 mov	 eax, DWORD PTR [esi]

; 41   : 				*aligned_dst++ = *aligned_src++;
; 42   : 				*aligned_dst++ = *aligned_src++;
; 43   : 				*aligned_dst++ = *aligned_src++;
; 44   : 				count -= BIGBLOCKSIZE;

  00057	83 e9 10	 sub	 ecx, 16			; 00000010H
  0005a	89 02		 mov	 DWORD PTR [edx], eax
  0005c	8b 44 13 04	 mov	 eax, DWORD PTR [ebx+edx+4]
  00060	89 42 04	 mov	 DWORD PTR [edx+4], eax
  00063	8b 46 08	 mov	 eax, DWORD PTR [esi+8]
  00066	89 42 08	 mov	 DWORD PTR [edx+8], eax
  00069	8b 46 0c	 mov	 eax, DWORD PTR [esi+12]
  0006c	83 c6 10	 add	 esi, 16			; 00000010H
  0006f	89 42 0c	 mov	 DWORD PTR [edx+12], eax
  00072	83 c2 10	 add	 edx, 16			; 00000010H
  00075	4f		 dec	 edi
  00076	75 dd		 jne	 SHORT $LL6@memmove

; 45   : 			}
; 46   : 
; 47   : 			/* Copy one long word at a time if possible.  */
; 48   : 			while (count >= LITTLEBLOCKSIZE)

  00078	8b 5d 08	 mov	 ebx, DWORD PTR _destination$[ebp]
  0007b	83 f9 04	 cmp	 ecx, 4
  0007e	72 15		 jb	 SHORT $LN3@memmove
  00080	8b f9		 mov	 edi, ecx
  00082	c1 ef 02	 shr	 edi, 2
$LL4@memmove:

; 49   : 			{
; 50   : 				*aligned_dst++ = *aligned_src++;

  00085	8b 06		 mov	 eax, DWORD PTR [esi]

; 51   : 				count -= LITTLEBLOCKSIZE;

  00087	83 e9 04	 sub	 ecx, 4
  0008a	89 02		 mov	 DWORD PTR [edx], eax
  0008c	83 c6 04	 add	 esi, 4
  0008f	83 c2 04	 add	 edx, 4
  00092	4f		 dec	 edi
  00093	75 f0		 jne	 SHORT $LL4@memmove
$LN3@memmove:

; 52   : 			}
; 53   : 
; 54   : 			/* Pick up any residual with a byte copier.  */
; 55   : 			dst = (char*)aligned_dst;
; 56   : 			src = (char*)aligned_src;
; 57   : 		}
; 58   : 
; 59   : 		while (count--)

  00095	85 c9		 test	 ecx, ecx
  00097	74 13		 je	 SHORT $LN45@memmove
  00099	2b f2		 sub	 esi, edx
  0009b	eb 03 8d 49 00	 npad	 5
$LL2@memmove:

; 60   : 		{
; 61   : 			*dst++ = *src++;

  000a0	8a 04 16	 mov	 al, BYTE PTR [esi+edx]
  000a3	8d 52 01	 lea	 edx, DWORD PTR [edx+1]
  000a6	88 42 ff	 mov	 BYTE PTR [edx-1], al
  000a9	49		 dec	 ecx
  000aa	75 f4		 jne	 SHORT $LL2@memmove
$LN45@memmove:
  000ac	5f		 pop	 edi
  000ad	5e		 pop	 esi

; 62   : 		}
; 63   : 	}
; 64   : 
; 65   : 	return destination;

  000ae	8b c3		 mov	 eax, ebx
  000b0	5b		 pop	 ebx

; 66   : }

  000b1	5d		 pop	 ebp
  000b2	c3		 ret	 0
_memmove ENDP
_TEXT	ENDS
END
